###
# Faster_RCNN: Towards Real-Time Object Detection with Region Proposal Networks
###
# Faster_RCNN可以做检测，但绝不仅仅做物体检测，人脸检测等等，一切的检测或者识别都可以基于这个框架来完成
# 而且Faster_RCNN的代码和模型以及数据全部都在它的Git上面公开了的

# 1. Region proposal algorithms(可以理解中文名字成为: 提取框的算法) 
# (1) 图像中的多目标检测，并且分类的前提以及最困难的是找到一个框去框住目标，其实后面的分类却并不难。
# (2) 难点在于：框的形状和大小都是不同的！
# (3) 对于框的提取办法：把几百个框通过一个共享的卷积网络去做分类
# (4) 这篇文章最厉害的地方在于构建的是一个End to End的模型，所有的其他内容都是在CNN之上的！  

# 2. 论文中提到当时三种目标检测的算法 multiple scaled images(anchor boxes), multiple filter sizes, multiple references都不怎么样。

# 3. 文章提出的办法是基于"anchor"(中文直译为:矛)。
# (1) 图像先经过一个CNN(如VGG16)得到输出"特征图'(feature_map)。
# (2) 根据"感受野"可以知道: 如果得到的feature_map尺寸为50*50，feature_map的每个1*1对应了原图的16*16像素大小的部分(因为VGG16有4个池化层)。
#     得到结论: feature_map上的点可以对应原始图像中的某块区域 --> 框住了feature_map的点就框住了原始图像的某区域 --> 用若干框去框住feature_map的点
# (3) 这种好处在于：(3.1) 输入只是一张原始图片！可以完成端到端的模型！
#                  (3.2) 前面的整个CNN部分对于所有的框来说都是共享的！

# 4. Region Proposal Network
# (1) 这个层就是负责"3.(2)"中的办法的实现:负责对于feature_map里面的点产生K个框(文章中K=9)
# (2) RoI_Pooling负责把不同尺寸的框都搞成统一大小的(文章中是7*7)，这样才可以连接全连接层！
###
# (2.1) 输入图像经过VGG16得到固定尺寸的feature_map, 上面的每一个点都产生了9个anchor,这些anchors的尺寸各不相同！
#       由于每个anchor都要再经过conv之后连接到FC上去(这后面的conv+FC就是负责产生分类或回归任务的网络)！
#       如果不同的尺寸的anchors进入conv后得到的结果尺寸必然不同，也就无法连接fc层！
#       *** 所以RoI_Pooling的作用就是把在feature_map上得到的anchors池化成为统一尺寸！！！ ***
###
# (3) 输入是一个任意大小的图像(长边不超过1000), 输出是框的四个顶点的坐标(回归)以及objectness_score(负责判断框内是否是物体)(二分类)
# (4) 输入图像的大小只会影响anchor_boxes的数量
# (5) feature_map上使用滑动窗口的办法去找到全部点的anchor_boxes


# 5. 整个架构分成两个模块，一个conv负责产生proposal(feature_map里面的一个点对应的K个anchor_boxes),另一个conv负责分类
# (1) 可以做成End_to_End结构

# 6. 损失函数 --> 具体见笔记本61
# (1) 一共有4个Loss：RPN层的Classification_loss和Bounding_box_regression_loss
#                   RoI_pooling层的lassification_loss和Bounding_box_regression_loss
# (2) Bounding_box_regression_loss是二分类即判断是否是前景的损失函数！                  

# 7. 总结
# (1) 图像先输入到CNN中 --> 得到feature_map
# (2) feature_map里面的每个点 --> 获取K个anchor_boxes
# (3) anchor_boxes经过RPN的cls和bbox回归得到proposal(这里的cls只针对anchors里面的目标是否为前景进行分类！)
# (4) 不同尺寸的proposal被喂入RoI_pooling --> 得到固定大小的proposal
# (5) 固定大小的proposal投入到classifier --> 得到最终结果(这里的cls对proposal里面的前景的具体类型进行分类！)

# 8. 代码分析
# (1) 2*9=18(objectness_score*num_of_anchor_boxes); 4*9=36(四个顶点坐标*num_of_anchor_boxes)这里的36和18都是网络根据loss_func以及label
      在不断的拟合学习过程中慢慢学会的，在实际的网络的代码里面，根本没有框的代码！也就是说给定要学习36个坐标数据，两个二分类数据，在拟合的过程中
      网络才慢慢明白要找的内容是什么 ----> 这一段话应该是深度学习的精髓，对于网络的学习过程和人类的思考过程的区别要明白！人类设计的有实际意义
      的结构(比如这里的框),在网络的面前只是一种学习的趋势，它不会在代码里面体现，只是在拟合的过程中慢慢达到人类的设计要求!
###
# 独立阅读Faster_RCNN论文总结！
###
#1. 先进的物体检测网络都依赖region proposal algorithms比如Fast_RCNN
#2. 但是框住物体的框的计算寻找却是瓶颈
#3. Faster_RCNN提出了region_proposal_network去找框，基本可以把找框的计算降到可以忽略
#4. 整个模型是End2End结构
#5. Faster_RCNN使用的最后的检测模型是Fast_RCNN的
#6. 通过使用非常深的VGG16提取特征，可以让模型的速度达到每秒5帧
#7. 以前的目标检测模型产生proposal的办法是Selective_Search
#   (1) Selective_Search: 计算相似度，包括颜色，纹理，尺寸,交并比
#8. 图1的介绍： image金字塔; filter金字塔； 以及RPN的anchor
#9. proposal: 一个提议框，对其做回归和分类
#10. Deep Networks for Object Detection: 像RCNN这样利用CNN去做端到端的目标检测被证明是很有效的
#11. 图2的介绍：Faster_RCNN的整体架构由两部分构成
#（1）RPN
#（2）Fast_RCNN的detector
#12. A Region Proposal Network (RPN) takes an image
#(of any size) as input and outputs a set of rectangular
#object proposals, each with an objectness score.
#13. 论文的VGG16用的FCN，具体操作是取消了原VGG16的conv3后面的FC层，增加了conv4,conv5以得到feature_map
#14. 共享卷积（共享了提取feature_map的VGG16）
#15. 论文的feature_map如果用ZF（5层conv）是256*256；如果用VGG16（13层）
#16. VGG16的Multi_scale训练：由于VGG16的输入是224*224，如果输入的图像尺寸不满足，
#    就把图像的短边缩放到大于224的某个值，然后提取224*224来训练
#17. 把特征图喂入RPN的时候，感受野对于ZF是171*171，对于VGG16是228*228
#18. 由于在feature_map上面用3*3的filter以滑动的方式提取anchors，所以实际上是共享卷积的
#19. 根据论文3.1的最后一段的描述以及对应的图3，可以知道提取做reg和cls的anchors是用3*3的filter然后
#    选中中间的点(所以论文里面说一个3*3的卷积核和两个1*1的卷积核(这两个分别对应reg和cls))
#20. (论文标题3.1.1 Anchors)如果有K个可能的proposal（论文中可能的proposal就是anchor），
#    对于reg就有4K个输入来表示这K个box(因为采用的是4元组),并且cls的话就有2K个scores来表示前景或者背景
#    并且在实验中，对于每一个3*3的滑动卷积核的中心点，K=9，因为是3种长宽比和3个基础长度一共9种组合
#    （长宽比有1:1，1:2,2:1）
#21. （论文标题Translation-Invariant Anchors->平移不变的anchor）
#     anchor的平移不变性可以减少RPN的输出规模，比如mutilbox就要输出800*(4+1),
#     而本文用的anchor的话输出只有（4+2）*512'
#22. (Multi-Scale Anchors as Regression References)
#    产生proposal的办法按照图1有以前的两种老的:image金字塔(有效但是费时)和filter金字塔(一般是两者结合使用)
#    而文章用的是多宽高比，多基础尺寸的anchor办法，特点是：
#    多尺寸的输入，单一尺寸的feature_map和filter，并且共享卷积！！！
#23. (3.1.2 RPN的Loss Function)
#    (1) RPN的score:前景和背景(正向和负向)两个标签
#    (2) 正向标签:最高IoU或者0.7的IoU(与GT的)
#    (3) 通常选择高于0.7的IoU是可以作为正向标签的，但是少数情况下是找不到高于0.7的！所以还是选择最高IoU
#    (4) 如果IoU低于0.3分配负标签
#    (5) 公式1为RPN的Loss_Function -> 要解释Lreg用的损失函数:SmoothL1(比较重要)
#    (6) 公式1的解释:Pi,Pi*,Ti,Ti*,Lcls,Lreg,(Pi*)*Lreg(乘Pi*是为了让正样本有Lreg并且负样本没有Lreg)
#    (7) 平衡Lcls和Lreg的权重(纳姆达)
#    (8) bounding_box的四元组:X,Y,W,H
#    (9) 公式2: X(proposal), Xa(anchor), X*(gt);Y,W,H类似！
#24. (3.1.3 Training RPNs)
#    (1) 一个mini-batch有256个anchors
#    (2) mini-batch里面的正负样本比例为1:1
#    (3) 前60K个mini-batch学习速率为0.001
#    (4) 后20K个mini-batch学习速率为0.0001
#    (5) momentum = 0.9 ----------------------------> momentum:动量梯度下降，第二门课2.6，计算梯度的指数加权平均来更新权重，一般就设置为0.9
#    (6) 权重衰减0.0005
#25. (3.2 Sharing Features for RPN and Fast R-CNN)
#    由于论文模型使用了RPN提取proposal以及Fast_RCNN做分类检测，所以要两个部分都要训练，采取了以下三种办法
#                                         * Fast_RCNN的分类检测模块从代码上面看来也就是RoI->FC->Relu->dropouy->cls:Softmax以及bbox:smoothL1*        
#    (1) Alternating training: 分开训练RPN以及Fast_RCNN的目标检测部分
#    (2) Approximate joint training：统一RPN和Fast_RCNN一起作训练
#    (3) Non-approximate joint training：RPN的预测结果送入RoI_Pooling层进行统一尺寸，然后再送入Fast_RCNN 
#                                                         * RoI_Pooling的设计可以见Fater_RCNN的代码 *
#26. (3.3 Implementation Details)论文的实现细节   
#    (1) 使用单一尺度的图像训练和测试region_proposal以及object_detection
#    (2) 缩放图像的短边到600像素值
#    (3) 不管使用的是ZF或者VGG16,由于感受野的原因,在feature_map上密集的滑动(一个像素一个像素的移动)filter,最后对应在输入图像上面的步幅是16pixel
#    (4) 宽高比三种有1:1,1:2,2:1以及三种基础尺寸128,256,512
#    (5) 训练期间丢弃了全部的跨界anchor
#    (6) 典型的1000*600的输入图像产生了约20000(60*40*9)个anchor，忽略跨界anchor的时候大约每张图像有6000个anchor
#    (7) 测试中把跨界proposal剪切到图像边界
#    (8) 在RPN的cls中对proposal采用了NMS
#    (9) IoU阈值为0.7，每个图像的proposal大约有2000个
#    (10)NMS不会降低最终的准确性，但是会大大减少proposal的数目！
#27. (Sensitivities to Hyper-parameters)
#    (1) 多anchor(9个)相较于单anchor有3%-4%的mAP下降
#    (2) (纳姆达)影响不敏感
#28. 结论
#    We have presented RPNs for efficient and accurate region proposal generation. 
#		By sharing convolutional features with the down-stream detection network, the region proposal step is nearly cost-free. 
#		Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates. 
#		The learned RPN also improves region proposal quality and thus the overall object detection accuracy.     
