###
# Detecting Text in Natural Image with Connectionist Text Proposal Network (CTPN)
###

# 0.Abstract
###########################################################################################################################
# 翻译:
# 我们提出了一种新颖的连接主义文本提议网络（CTPN），它能够准确定位自然图像中的文本行。
# CTPN直接在conv_feature_map中检测一系列fine_scale_text_proposal中的文本行。
# 我们开发了一种垂直anchor机制，它可以共同预测每个固定宽度proposal的位置和文本/非文本分数，从而大大提高了定位精度。
# 顺序proposal自然地通过循环神经网络连接，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。
# 这使得CTPN可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。
# CTPN在多尺寸和多语言文本中可靠地工作，无需进一步的后处理，从之前的自底向上方法需要多步骤后过滤。
# using the very deep VGG16 model
###########################################################################################################################
# (1) 用一种竖直anchor机制去预测text/non-text
# (2) 结构:CNN -> Proposal -> RNN
# (3) CTPN可以在多语言多尺寸的文本检测中工作，并且采用自底向上的方法

# 1.Introduction
###########################################################################################################################
# 翻译:
# 计算机视觉中阅读自然图像中的文本包括两个子任务：文本检测和识别
# 文本模式的大变化和高度混乱的背景构成了准确文本定位的主要挑战。
# 我们利用强大的深层特征直接在卷积地图中检测文本信息。
# 我们开发text_anchor机制，可精确预测精细文本位置。
# 然后，提出了一种网内循环体系结构，用于按顺序连接这些fine_scale_text_proposal，从而允许它们编码丰富的上下文信息。
# 深度卷积神经网络（CNN）最近已经基本实现了一般物体检测。
# 最先进的方法是Faster_Region_CNN（R-CNN）系统，其中提出了Region_Proposal_Network以直接从卷积特征映射生成高质量类别不可知对象提议。
# 然后将RPN_proposal输入Fast_R_CNN模型进行进一步的分类和优化，从而实现通用对象检测的最新性能。
# 然而，将这些通用对象检测系统直接应用于场景文本检测是困难的，这通常需要更高的定位精度。
# 在通用对象检测中，每个对象都有一个明确定义的封闭边界[2]，而文本中可能不存在这样一个明确定义的边界，因为文本行或单词由许多单独的字符或笔划组成。
# 全面阅读文本是一项细致的识别任务，需要正确的检测来覆盖文本行或文本的整个区域。
###########################################################################################################################
# (1) 两个核心子任务:文本检测和文本识别
# (2) 在文本精确定位中的挑战主要是:各式各样的文字形式以及非常杂乱的背景干扰
# (3) CTPN是Faster_R_CNN针对于文本检测中的一种发展，因为文本检测需要更加的精细，把文本区域都包含进去
# (4) 端到端的模型

# 1.1 Contributions
###########################################################################################################################
# 图1：（a）连接主义文本提议网络（CTPN）的体系结构。
# 我们通过VGG16模型的最后卷积图（conv5）密集地滑动3×3空间窗口。
# 每行的顺序窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）用作256D BLSTM（包括两个128D LSTM）的输入。
# RNN层连接到512D完全连接层，接着是输出层，它共同预测k个anchor的文本/非文本分数，y轴坐标和侧向细化偏移量。
# （b）CTPN输出连续的固定宽度fine_scale_text_proposal。每个框的颜色表示文本/非文本分数。
###########################################################################################################################
# (1) CTPN可以在卷积层中定位文本
# (2) *** 文章在图1中提出了CTPN的架构, 具体见笔记本 ***
# (3) CTPN把文本检测用一堆小的竖直的框来完成
# (4) CTPN使用了BLSTM(双向LSTM)，在检测的过程中可以探索文本的上下文信息以提高检测水平


# 2.Related Work
###########################################################################################################################
# Text detection
# 过去在场景文本检测中的作品一直以自下而上的方法为主，一般建立在笔画或字符检测之上。
# 它们大致可以分为两类，基于连接元件（CC）的方法和基于滑动窗口的方法。
# 基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等）将文本像素粗略地分组为笔划或候选字符。
# 基于滑动窗口的方法通过在图像中密集地移动多尺度窗口来检测候选字符。
# 字符或非字符窗口通过预先训练的分类器，使用手动设计的特征或最近的CNN特征[16]进行区分。
# 但是，这两组方法通常都会受到字符检测性能较差的影响，导致在组件过滤和文本行构建步骤中出现累积错误。
# 此外，强大地过滤非字符组件或自信地验证检测到的文本行本身就很困难。
# 另一个限制是滑动窗口方法在计算上昂贵，通过在大量的滑动窗口上运行分类器。
# Object detection
# 卷积神经网络（CNN）最近已经基本上推进了通用物体检测。
# 一种常见的策略是通过使用廉价的低级特征来生成许多对象提议，然后应用强CNN分类器来进一步对生成的提议进行分类和细化。
# 选择性搜索（SS）[4]产生类别不可知的对象提议，是目前领先的对象检测系统中应用最广泛的方法之一，如区域CNN（R-CNN）[6]及其扩展[5]。
# 最近，Ren等人[25]提出了一种更快的R-CNN系统用于物体检测。
# 他们提出了一个区域提议网络（RPN），它可以直接从卷积特征映射生成高质量的类别不可知对象提议。
# RPN通过共享卷积计算是快速的。然而，RPN建议不具有区别性，并且需要通过额外的昂贵的CNN模型（例如Fast R-CNN模型）进一步改进和分类。
# 更重要的是，文本与一般对象有很大不同，因此很难直接将通用对象检测系统应用于这个高度特定领域的任务。
###########################################################################################################################
# 2.1 Text Detction
# (1) 常见的基于字体或笔画的文本检测中使用的方法包括:CCs以及Sliding-window(滑动窗口)办法
# (2) CCs通过区分文本和非文本像素的低级特征如强度，颜色，梯度等来实现，很粗略
# (3) 滑动窗口的办法非常复杂因为窗口很多，大量重复计算，而且准确率不咋样
# 2.2 Object detection
# (1) CNN通过对低级特征的提取和强大的分类器来实现了物体的检测
# (2) Selective_Search(选择性搜索)是现在目标检测中应用广泛的办法，主要是R-CNN及其扩展
# (3) 最关键的是文本检测有它的特殊性，很难直接用Faster-R-CNN这种模型直接用在文本检测中去


# 3.Connectionist Text Proposal Network
###
# 本节介绍Connectionist Text Proposal Network（CTPN）的详细信息。
# 它包括三个关键贡献，使文本本地化可靠和准确：检测精细提案中的文本，经常性连接文本提议和侧面改进。
###
# (1) 三个办法实现: 使用fine_scale(精细检测的，通俗说就是很细很瘦高的)的proposal
#                  Recurrent_connectionist文本proposal(也就是后面连接了一个双向LSTM)
#                  side-refinement侧面改进
# 3.1 Dectecting Text in Fine-scale Proposal
###########################################################################################################################
# 类似于区域提议网络（RPN）[25]，CTPN本质上是一个完全卷积网络，允许任意大小的输入图像。
# 它通过密集地滑动卷积特征图中的小窗口来检测文本行，并输出一系列精细尺度（例如，固定的16像素宽度）文本提议，如图1（b）所示。
# 我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。 CTPN的体系结构如图1（a）所示。
# 我们使用一个小的空间窗口3×3来滑动最后的卷积层的特征图（例如，VGG16的conv5）。
# conv5特征图的大小由输入图像的大小决定，而总步幅和感受域分别固定为16和228像素。
# 网络体系结构决定总步幅和接受范围。
# 在卷积层中使用滑动窗口允许它共享卷积计算，这是减少昂贵的基于滑动窗口的方法的计算的关键。
# 通常，滑动窗口方法采用多尺度窗口来检测不同尺寸的对象，其中一个窗口尺度被固定到相似尺寸的对象。
# 在[25]中，Ren等人提出了一种有效的锚点回归机制，允许RPN使用单尺度窗口检测多尺度对象。
# 关键的见解是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和纵横比的对象。
# 我们希望将这种有效的锚机制扩展到我们的文本任务。
# 然而，文本与普通对象基本不同，它们通常具有明确的封闭边界和中心，从而甚至可以从它的一部分推断整个对象[2]。
# 文本是一个没有明显封闭边界的序列。
# 它可能包含多层次的组件，如笔划，字符，单词，文本行和文本区域等，这些组件之间没有明确区分。
# 文本检测是在单词或文本行级别定义的，因此通过将其定义为单个对象（例如检测单词的一部分）可能很容易进行错误的检测。
# 因此，直接预测文本行或单词的位置可能很难或不可靠，因此很难获得满意的准确性。图2显示了一个例子，RPN直接训练用于定位图像中的文本行。
# 我们寻找一种文本的独特属性，它能够很好地概括所有级别的文本组件。
# 我们观察到由RPN进行的单词检测很难准确预测单词的水平面，因为单词中的每个字符都是孤立的或分离的，这使得混淆查找单词的开始和结束位置。
# 显然，文本行是一个序列，它是文本和通用对象之间的主要区别。
# 将文本行视为一系列精细文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16像素的文本块。
# 每提案可能包含单个或多个笔划，字符的一部分，单个或多个字符等。
# 我们认为，通过固定其较难预测的水平位置来预测每个提案的垂直位置会更准确。
# 预测对象的4个坐标的RPN相比，这减少了搜索空间。
# 我们开发了垂直锚定机制，可以同时预测每个精细提案的文本/非文本分数和y轴位置。
# 检测一般固定宽度的文本提议比识别隔离字符更加可靠，该隔离字符容易与字符或多个字符的一部分混淆。
# 检测一系列固定宽度文本提议中的文本行也可以可靠地在多个比例和多个纵横比的文本上可靠地工作。
# 为此，我们设计如下的精细文本提案。我们的探测器密集调查conv5中的每个空间位置。
# 文本提议被定义为具有16像素的固定宽度（在输入图像中）。
# 这等于通过conv5贴图密集地移动探测器，其中总跨度恰好为16像素。
# 然后，我们设计k个垂直锚来预测每个提议的y坐标。
# k个锚具有相同的水平位置，固定宽度为16个像素，但其垂直位置在k个不同高度下变化。
# 在我们的实验中，我们为每个提议使用10个锚，k = 10，其高度从输入图像中的11到273像素（每次÷0：7）变化。
# 明确的垂直坐标是通过建议边界框的高度和y轴中心来测量的。
# 我们计算相对于锚的边界框位置的相对预测的垂直坐标（v），其中v = fvc; vhg和v * = fvc *; vh * g分别是相对预测坐标和地面实况坐标。
# ca y和ha是锚箱的中心（y轴）和高度，可以根据输入图像预先计算。
# cy和h是输入图像中预测的y轴坐标，而c * y和h *是地面实况坐标。
# 因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为h×16的输入图像的边界框。
# 一般来说，文本提案在很大程度上小于228×228的有效接受范围。
###########################################################################################################################
# (1) CTPN使用的是16像素宽的精细文本proposal
# (2) *** 在feature_map(VGG16的conv5)中使用3*3的滑动窗口 ***
# (3) *** total_stride = 16, 感受野 = 228 ***
# (4) RPN直接用于文本检测的话容易出现问题，不能有好的检测效果,因为单词或者文字各个都是独立的，容易混淆他们的位置
# (5) 通过固定较难预测的水平位置来预测proposal的竖直位置
# (6) CTPN使用竖直的anchor来预测每个fine_scale_proposal的text/non-text分数和y轴位置
# (7) 精细文本proposal(fine-scale-text-proposal)设计
# (7.1) detector密集扫描整个feature_map
# (7.2) text_proposal在输入图像中具有16个像素的宽度，等效于feature_map里面的每个像素点的感受野宽度为16
# (7.3) *** 然后设计K个竖直的具有相同水平位置(?)的16个像素宽度的垂直位置的高度可变的anchor来为每个proposal预测y坐标 ***
# (7.4) 文章的实验中使用的是10个anchor，高度从11到273个像素点(每次除以0.7)
# (7.5) *** 垂直坐标(vertical coordinates)是通过proposal_bounding_box的高度和y轴中心来测量的 ***
# (7.6) 计算anchor的bounding_box的预测的相对垂直坐标(v)的公式见笔记本！
# (7.7) 每个text_proposal在输入图像中都有大小为h*16的bounding_box
# (7.8) 文章中说一般text_proposal的感受野尺寸都会大大小于228*228
# (7.9) anchor的竖直坐标由其高度以及中心点的y坐标确定
# (8) 检测处理(detction processing)设计
# (8.1) CONV使用的是VGG16模型,所以得到的feature_map尺寸为: W*H*C(conv5)(C:通道数量,W*H:特征图的尺寸)
# (8.2) detector使用3*3*C的窗口密集滑动过(应该理解成步长等于1)feature_map用来产生预测
# (8.3) *** 对于每个预测，水平位置和K个anchor的位置是固定的，可以把feature_map里面的窗口位置映射到输入图像来预先计算 ***
# (8.4) 在每个窗口输出K个anchor的text/non-text的scores和y坐标
# (8.5) 非极大值抑制使用的是text/non-text的score>0.7

# 3.2 Recurrent Connectionist Text Proposal
# (1) 由于文本有很强烈的上下文顺序特征，所以单独的处理proposal是不行的，所以要对每一个proposal进行相关的处理 --> 这是使用双向LSTM的原因！
# (2) *** CTPN在conv5上面增加了一个RNN，把每个窗口的卷积feature作为输入，并且在隐藏层Ht中循环更新其内部状态 *** --> 这部分的公式和解释见笔记本！！
# (3) *** 剩下的整个RNN层的设计相关见笔记本！！！ ***

# 3.3 Side-refinement
# (1) 文本行构建：
# (1.1) 给proposal Bi一个配对的邻居Bj定义为: Bj->Bi如果满足(1.2),(1.3),(1.4)的三个条件
# (1.2) Bj是Bi水平最近的
# (1.3) Bj和Bi的水平距离小于50个像素
# (1.4) 他们的垂直重叠值大于0.7
# (1.5) 如果两个proposal有Bj->Bi,并且Bi->Bj的话，就把这两个proposal分成一对(a pair)
# (1.6) 整个文本行就是由这一些连续的连接的pairs构成的
# (2) Side-refinement处理
# (2.1) 在文本检测中，如果两个水平边的proposal没有检测出来会导致很严重的问题, Side-refinement就是为了精确处理这个问题的办法
# (2.2) *** Side-refinement的具体处理办法见笔记本！！！ ***

# 3.4 Model Outputs and Loss Function
# (1) 如图1所示，在最后一个FC层连接了三个output，分别是: 2K个vertical_coordinates(竖直坐标); 2K个scores; K个side-refinement(边界精细偏移量)
# (2) 使用多任务学习来联合优化模型参数
# (3) *** 损失函数见笔记本！！！ ***

# 3.5 Training and Implementation Details(训练和实施细节)
# (1) 使用标准的反向传播和随机梯度下降(stochastic gradient descent,SGD)来进行End2End的训练
# (2) CTPN与RPN类似，训练样本是anchor,其位置可以在img中预先计算，以便找到相应的GT的label
# (3) 训练标签(training label)
# (3.1) 在text/non-text的分类中，把二元标签分配给正(text)anchor和负(non-text)anchor
# (3.2) 正anchor要满足下面的(3.3)和(3.4)
# (3.3) 和GT要有大于0.7的IoU
# (3.4) 和GT有最高的IoU
# (3.5) 由(3.4)可以知道即使是一个非常小的文本图像也会分配一个正anchor
# (3.6) 负anchor被定义成:和GT的IoU小于0.5
# (4) 训练数据(training data)
# (4.1) 在训练过程中，mini-batch里面的每个samples都是从一张图片里面收集的
# (4.2) mini-batch中的anchor的数量: Ns=128,正负样本比例为1:1
# (4.3) 如果正样本的数目少于64，则会用负样本填充到mini-batch中去
# (4.4) *** 文章里面把图像的短边都搞成600的同时保持高宽比 ***
# (5) 实施细节(Implementation detail)
# (5.1) 文中的VGG16是在ImageNet数据上预先训练好的(这个是惯例)
# (5.2) RNN层和Output_layer用的是(均值为0，标准差为0.01)的高斯分布随机权重初始化
# (5.3) momentum=0.9
# (5.4) 0.0005权重衰减
# (5.5) 在第一次的16K次迭代中学习率等于0.001，随后再以0.0001的学习率进行4K次迭代

# 4.Experimental Results and Discussions
# (1) 文章中的实验是先单独验证每个组件的效率，比如fine-scale_text_proposal_detection和RNN

# 4.1 Benchmarks and Evaluation Metric(基准和评估指标)
# (1) 原文:The Multilingual scene text dataset is collected by [24].
# (1) 翻译:  多语言场景文本数据集由[24]收集

# 4.2 Fine-Scale Text Proposal Network with Faster R-CNN
# (1) fine-scale_text_proposal针对文本识别提升了准确率

# 4.3 Recurrent Connectionist Text Proposals
# (1) RNN在模糊的文本中检测效果显著

# 4.4 Comparisons with state-of-the-art results(与最先进的结果进行比较)

