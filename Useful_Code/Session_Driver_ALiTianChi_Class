# Tensorflow编写代码是时候是通过符号式的方法去构造Graph的过程，实际上整个程序在sess.run()之前是都没有执行的！

#                                    Serialized Graph(排列好Graph)                        
# 单机中Tensorflow运行模型：   Client ------------------------------> Server                          
#                                |    In-process(进程内部)              |                                
#                           (用户写的代码)                     (Tensorflow Runtime)                            
# 在Client把Graph序列化好以后，通过进程内部通信的方式，调用C++的Runtime                                           
                                                                                                             
# 分布式：
# 1. 例子:  with tf.device('/cpu:0')
#         Serialized Graph           Register sub-graph
# Client ------------------> Server --------------------> Workers
#              GRPC协议         |          GRPC协议
#                      (负责拆解Graph并分配)                              

# 源码里的定义：
#  class Session():
#     def __init__(      
#          targer = ''            // Client通过GRPC协议发送Graph到的目的地址
#          graph  = None          // 默认是None，即当前程序内部定义的全部的Graph     
#          config = None) ...     // 指定一些运行配置

# 如何高效的写代码(老师的经验)(在理解tf的API后才会起作用):
# 1. 如果允许, 数据和计算尽可能"离得近"
#    - 距离: device之间数据传输所经过的路径
#    - with ops.colocate_with(param)                    // 让这个函数下面定义的所有操作都和param位于同一个device上
# 2. 正确的使用tf.Variable()
#    - 考虑用tf.get_variable()代替                       // 见笔记本后第12条！
# 3. 合理程序需要的硬件资源，进而布局代码
#    - 分布式只是不得已而为之的手段，不是越多分布式越好
#    - 要估计显存大小，内存大小

# 数据I/O的几种方式
# 1. Feed方式
#    - placeholder + feed.dict 
#    - 简单，快速，例子多，但是性能有损失(特别是对于分布式的情况,数据拷贝开销太大)
#    - 可以不依赖文件系统
# 2. Preload方式
#    - 办法1：constant(用这种常量的方式，将数据预先装载在Graph里面)
#    - 办法2：将数据先run进内存，以后要用的时候直接去取
#    - 仅适用于小量数据
# 3. Pipeline方式                                        // Graph由Node组成
#    - XXXReader, Op                                     // 由某类型的Reader读出后发给Op
#    - Graph流水线执行，需要数据时自动拉取
#    - 好处在于也是Graph方式的，即在run之前没有任何数据的流动，并且在run之后，也不用去一次次的feed数据，开销就要小一些

# Queue机制       
# 1. 生产者和消费者的中间介质           // 1的模型图见笔记本后第13条！
# 2. tf里独立于sess.run()(主线程)执行,即虽然Queue也是依靠sess.run()启动，但是启动之后就在自己独立的线程里面执行了
# 3. 异步I/O的实现！
#    - reader.read(queue)
#    - tf.train.batch()   ----> 当Queue里的数据量达到batch_size时才返回，作为batch

# 加速数据I/O --> 目标: I/O不再成为计算的瓶颈 --> 原因: GPU是最贵的计算资源，好的算法应该通过设计让CPU维持在较高水准
# 阶段1：将小文件压缩成大文件，在深度学习中，将原始的零散的音频或者图片的数据压缩成为一个整的大文件
# 阶段2: 尽量使用Pipeline方式读取数据，尤其分布式的情况
# 阶段3：考虑用reader.read_up_to()代替tf.train.batch()
# 阶段4：使用PAI-TF的Cache功能
# 例子： reader.read_up_to(n)(Pipeline) VS reader.read() + tf.train.batch(n)(Feed)
#  |                 |                                    |
#  |             构造文件队列                          构造文件队列
#  |                 |                                    |
#  |            构造reader对象                        构造reader对象
#  |                 |                                    |
#  |           从文件中读取n条数据                    从文件中读取1条数据
#  |                 |                                    |
#  |              返回数据                           将该数据加入数据队列  <--- 多线程enqueue 
#  |                                                      |
#  |                                               如果数据队列长度超过n, <--- 单线程dequeue
#  |                                                   则从队列返回结果    
#  |
# Request timeline


