# 1. ML_Week1:引言；单变量线性回归；线性代数回顾
(1) SVM可以处理无限多种特征的情况。
(2) 常见的监督学习：回归和分类。
(3) 监督学习：给了label或者说gt。
(4) 无监督学习:只给数据集，没有给label，聚类算法是常见的无监督学习算法。
*(5)假设函数:对于线性回归就是h(x)=瑟塔的转置*X，对于逻辑回归二分类情况就是sigmoid函数！--> 这里在之前理解的不是很好！没有吧分类器与假设函数的形式联系！
(6) 线性回归的成本函数(P17)：对于大多数线性回归问题都用平方误差函数作为成本函数！
(7) 批量梯度下降公式(P21)，每一次都同时让所有的参数减去学习率城西成本函数的导数,并且更新也是同时(P22)的！
(8) 学习率：太小会导致迭代过慢，太大可能导致无法收敛甚至发散(因为这样可能会越过局部最优点，通过图可以很直观的看出来)
(9) 批量梯度下降：在梯度下降的每一部中，都用到了所有的训练样本。
(10) 矩阵维数：行数 * 列数。

# 2. ML_Week2:多变量线性回归；Octave教程
(1) 多变量的假设方程：h(P36)多个特征与多个参数的乘累加！
(2) 多变量的梯度下降：与单变量的没什么差别！出了假设方程变化！参数的梯度下降更新公式也是一样的！ -->(推导公式P37或者本子)
(3) 特征缩放：尽量让特征的尺度都缩放到[-1,1]这样参数的图像就是越圆的，而不是扁的！圆的利于快速收敛！
(3.1) 如果采用多项式回归模型，在运行梯度下降算法之前，特征缩放非常有必要！

# 3. ML_Week3:逻辑回归;正则化
(1) 逻辑回归就是是分类问题的一种算法！要预测的y是一个离散值！
(2) 由于常见的二分类需要分类器的输出是(0,1)所以假设方程: h = g(参数矩阵*数据矩阵)，这里的y=g(x)就是常见的2分类器函数！比如sigmoid,tanh,Relu等等。
(3) sigmoid函数表达式和图像(p91)
(4) 逻辑回归的成本函数(p95)：Cost(h(x),y) = -y*log(h(x))-(1-y)*log(1-h(x))
(5) 逻辑回归中也用的是梯度下降法来最小化成本函数！虽然看上去更新公式一样！但是实际上由于有了分类函数g(x)所以实际上是不一样的！ -->(推导公式P96或者本子)
(5.1) 这里的公式推导的时候用的是sigmoid分类器！
(6) 完整的逻辑回归算法：1. 列出逻辑回归的成本函数-->最小化成本函数以拟合出让成本函数最小的参数！(通过梯度下降法去最小化成本函数)
                      2. 通过反复的更新每个参数,更新办法：用参数减去学习率*偏微分
                  *** 3. 如(***P100***这一页很重要)所说！自行推导后发现逻辑回归(sigmoid)与多元线性回归推导出来的更新参数的公式是一样的！*** 
                  *** 4. 但是实际上他们确实是不同的东西！因为推导过程都不一样 ***
                  *** 5. 牢记公式和推导过程！！！ ***
(7) 正则化：使用正则化处理过拟合，就是通过保留所有特征但是减少参数的大小来实现！              

# 4. ML_Week4：神经网络简述
(1) 权重矩阵尺寸：下一层的激活单元数为行数，本层的激活单元数为列数。

# 5. ML_Week5：神经网络的学习
(1) *** 为计算损失函数的偏导数，需要用反向传播算法 ***，从最后一层误差开始反向求出各层的误差，直到倒数第二层。
    实际上的意义在于求出损失函数的偏导数才可以来更新参数！而且注意FP中不仅要求出最后的预测值！还需要各层的预测值！
    因为在BP的时候要用反向得到的值和预测值来做对比！(最后一层用的是label和预测值作对比！其他层用的是反向计算出的值和预测值作对比！)
    
# 6. ML_Week6: 应用机器学习的建议
(1) 决定下一步做什么：改进算法的性能应该做的事情。
(2) 判断是否过拟合：测试集，训练集，交叉训练集。
(3) 偏差大->欠拟合，方差大->过拟合。
**(4) 查准率Precision：TP/(TP+FP) -> 预测为真中实际为真的概率
**(5) 召回率Recall：   TP/(TP+FN) -> 实际为真中预测为真的概率

# 7. ML_Week7：SVM

# 8. ML_Week8：聚类
(1) K-mean算法
(1.1) K-mean的最小化问题就是最小化所有的数据点与其所关联的聚类中心点之间的距离值之和。

# 9.ML_Week9：异常检测

# 10.ML_Week10：大规模机器学习







# 补充:
(1) 搞清楚线性回归和逻辑回归里面的：假设函数，成本函数，损失函数以及梯度下降公式各是什么！
(1.1) 逻辑回归的假设函数就是分类器！比如二分类使用的sigmoid！
