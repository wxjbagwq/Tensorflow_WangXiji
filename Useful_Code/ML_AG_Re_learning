# 1. ML_Week1:引言；单变量线性回归；线性代数回顾
(1) SVM可以处理无限多种特征的情况。
(2) 常见的监督学习：回归和分类。
(3) 监督学习：给了label或者说gt。
(4) 无监督学习:只给数据集，没有给label，聚类算法是常见的无监督学习算法。
*(5)假设函数:对于线性回归就是h(x)=瑟塔的转置*X，对于逻辑回归二分类情况就是sigmoid函数！--> 这里在之前理解的不是很好！没有吧分类器与假设函数的形式联系！
(6) 线性回归的成本函数(P17)：对于大多数线性回归问题都用平方误差函数作为成本函数！
(7) 批量梯度下降公式(P21)，每一次都同时让所有的参数减去学习率城西成本函数的导数,并且更新也是同时(P22)的！
(8) 学习率：太小会导致迭代过慢，太大可能导致无法收敛甚至发散(因为这样可能会越过局部最优点，通过图可以很直观的看出来)
(9) 批量梯度下降：在梯度下降的每一部中，都用到了所有的训练样本。
(10) 矩阵维数：行数 * 列数。

# 2. ML_Week2:多变量线性回归；Octave教程
(1) 多变量的假设方程：h(P36)多个特征与多个参数的乘累加！
(2) 多变量的梯度下降：与单变量的没什么差别！出了假设方程变化！参数的梯度下降更新公式也是一样的！ -->(推导公式P37或者本子)
(3) 特征缩放：尽量让特征的尺度都缩放到[-1,1]这样参数的图像就是越圆的，而不是扁的！圆的利于快速收敛！
(3.1) 如果采用多项式回归模型，在运行梯度下降算法之前，特征缩放非常有必要！

# 3. ML_Week3:逻辑回归;正则化
(1) 逻辑回归就是是分类问题的一种算法！要预测的y是一个离散值！
(2) 由于常见的二分类需要分类器的输出是(0,1)所以假设方程: h = g(参数矩阵*数据矩阵)，这里的y=g(x)就是常见的2分类器函数！比如sigmoid,tanh,Relu等等。
(3) sigmoid函数表达式和图像(p91)
(4) 逻辑回归的成本函数(p95)：Cost(h(x),y) = -y*log(h(x))-(1-y)*log(h(x))
(5) 逻辑回归中也用的是梯度下降法来最小化成本函数！虽然看上去更新公式一样！但是实际上由于有了分类函数g(x)所以实际上是不一样的！ -->(推导公式P96或者本子)
(5.1) 这里的公式推导的时候用的是sigmoid分类器！
(6) 完整的逻辑回归算法：1. 列出逻辑回归的成本函数-->最小化成本函数以拟合出让成本函数最小的参数！(通过梯度下降法去最小化成本函数)
                      2. 通过反复的更新每个参数,更新办法：用参数减去学习率*偏微分
                      






